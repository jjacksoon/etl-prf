# ğŸ›£ï¸ Pipeline de Dados PRF: Arquitetura de MedalhÃ£o com Docker & Postgres

Este projeto implementa um pipeline de dados (ETL) robusto para o processamento de dados histÃ³ricos de acidentes em rodovias federais brasileira (PRF), cobrindo o perÃ­odo de **2005 a 2025**. O foco principal Ã© a transformaÃ§Ã£o de grandes volumes de dados brutos e inconsistentes em um **Data Warehouse** otimizado para consultas SQL e BI.

## ğŸ—ï¸ Arquitetura do Projeto

O pipeline foi estruturado utilizando a **Medallion Architecture**, garantindo linhagem e qualidade dos dados em cada etapa do processo:

1.  **Bronze (Raw - Web Scraping):** Desenvolvi um extrator customizado em Python que realiza o **scraping** do portal de dados abertos da PRF. O script automatiza a navegaÃ§Ã£o, identifica os links de download por ano e realiza a ingestÃ£o dos arquivos CSV brutos, eliminando o trabalho manual e garantindo a escalabilidade do pipeline.
2.  **Silver (Cleaned):** Limpeza profunda, normalizaÃ§Ã£o de encodings (Latin-1 para UTF-8), tratamento de tipos de data e padronizaÃ§Ã£o de colunas. Armazenamento eficiente em formato **Apache Parquet**.
3.  **Gold (Curated):** UnificaÃ§Ã£o de mais de 20 anos de registros em uma Ãºnica tabela mestra (Single Source of Truth), otimizada para anÃ¡lise volumÃ©trica.
4.  **Load (Database):** Carga dos dados tratados em um banco de dados **PostgreSQL** orquestrado via **Docker**, permitindo acesso relacional performÃ¡tico.

## ğŸ› ï¸ Tecnologias e Ferramentas

* **Web Scraping:** `requests` / `BeautifulSoup` (ou a biblioteca que usamos)
* **Linguagem:** Python 3.11
* **Processamento de Dados:** Pandas
* **Linguagem:** Python 3.11
* **Processamento de Dados:** Pandas
* **Armazenamento Colunar:** Apache Parquet (PyArrow)
* **Infraestrutura:** Docker & Docker Compose
* **Banco de Dados:** PostgreSQL 15
* **Conectividade:** SQLAlchemy & Psycopg2
* **SeguranÃ§a:** Python-dotenv (VariÃ¡veis de ambiente)

## ğŸ“ Estrutura do RepositÃ³rio

```text
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ bronze/          # Arquivos CSV originais
â”‚   â”œâ”€â”€ silver/          # Arquivos Parquet limpos por ano
â”‚   â””â”€â”€ gold/            # Parquet Ãºnico e unificado
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extraction/      # Script de download (Bronze)
â”‚   â”œâ”€â”€ transformation/  # Scripts Silver e Gold
â”‚   â””â”€â”€ storage/         # Script de carga para Postgres
â”œâ”€â”€ .env                 # Credenciais (nÃ£o versionado)
â”œâ”€â”€ docker-compose.yml   # OrquestraÃ§Ã£o do banco de dados
â””â”€â”€ requirements.txt     # DependÃªncias do projeto
```

## ğŸš€ Como executar

### 1. Clonar e instalar

```
git clone [https://github.com/seu-usuario/et-prf.git](https://github.com/seu-usuario/et-prf.git)
cd et-prf
pip install -r requirements.txt
```

### 2. Configurar o Ambiente

Crie um arquivo .env na raiz:

```
POSTGRES_DB=db_acidentes
POSTGRES_USER=user_prf
POSTGRES_PASSWORD=sua_senha_segura
```

### 3. Subir a Infraestrutura
```
docker-compose up -d
```
### 4. Rodar o Pipeline

```
python src/extraction/extract_prf.py
python src/transformation/silver_transformation.py
python src/transformation/gold_unification.py
python src/transformation/load_to_postgres.py

```

##  ğŸ“Š Resultados e Performance

- **Escalabilidade:** Processamento de mais de 2,1 milhÃµes de registros.
- **OtimizaÃ§Ã£o:** ConversÃ£o de arquivos CSV pesados para Parquet, reduzindo o tempo de leitura e espaÃ§o em disco.
- **Integridade:** Tratamento de inconsistÃªncias histÃ³ricas em esquemas de dados que mudaram ao longo de duas dÃ©cadas.

## ğŸ‘¨â€ğŸ’» Sobre o Autor
Jackson Nascimento - Engenheiro de Dados em formaÃ§Ã£o | BI | Analytics

Projeto desenvolvido com foco em aprendizado real de engenharia de dados, indo alÃ©m de tutoriais e demonstrando capacidade de estruturar pipelines prÃ³ximos ao cenÃ¡rio profissional.

ğŸ”— LinkedIn: https://www.linkedin.com/in/jackson10/